# ğŸ§  Python Code Language Model (BPE + Word2Vec + BiLSTM)

This project explores how **natural language processing (NLP) techniques** can be applied to **source code**.  
We build a pipeline that:  
- **Tokenizes Python code and docstrings** using **Byte Pair Encoding (BPE)**  
- Learns **semantic embeddings** via **Word2Vec**  
- Trains a **BiLSTM Language Model** to predict and generate Python-like code  

Inspired by modern code assistants (like GitHub Copilot), this project demonstrates how smaller, classical models can still capture meaningful programming patterns.

---

## ğŸ“‚ Project Structure

```
GenerativeAI_Project/
â”œâ”€â”€ bpe/                # BPE tokenizer + trained models
â”‚   â”œâ”€â”€ bpe_tokenizer.py
â”‚   â”œâ”€â”€ bpe_code.pkl
â”‚   â””â”€â”€ bpe_doc.pkl
â”œâ”€â”€ word2vec/           # Word2Vec embeddings
â”‚   â”œâ”€â”€ word2vec.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ train_word2vec.py
â”‚   â””â”€â”€ word2vec_model.pt
â”œâ”€â”€ bilstm/             # BiLSTM language model
â”‚   â”œâ”€â”€ bilstm_model.py
â”‚   â””â”€â”€ bilstm_model.pt
â”œâ”€â”€ data/               # Dataset (âŒ not included in repo, see note below)
â”‚   â””â”€â”€ clean_dataset.csv
â”œâ”€â”€ notebooks/          # Analysis & evaluation notebooks
â”‚   â”œâ”€â”€ bpe_evaluation.ipynb
â”‚   â”œâ”€â”€ word2vec_evaluation.ipynb
â”‚   â”œâ”€â”€ bilstm_language_model.ipynb
â”‚   â””â”€â”€ bilstm_evaluation.ipynb
â”œâ”€â”€ main.py             # Integrated pipeline (BPE â†’ Word2Vec â†’ BiLSTM)
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # Project documentation
```

---

## âš ï¸ Notes on Excluded Files

- The `venv/` folder (virtual environment) is **not included** in this repo. Please create your own environment following the instructions below.  
- The `data/` folder is also **not included** because of dataset size.  
  - You will need to provide your own dataset of Python functions & docstrings.  
  - Expected format: `clean_dataset.csv` with two columns:  
    - `code` â†’ Python function code  
    - `docstring` â†’ corresponding natural language description  

---

## âš™ï¸ Setup Instructions

### 1. Clone the repo
```bash
git clone https://github.com/ibraheem8887/GenerativeAI_Project.git
cd GenerativeAI_Project
```

### 2. Create and activate a virtual environment
```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Add dataset
Place your dataset in the `data/` folder:
```
data/clean_dataset.csv
```

---

## â–¶ï¸ How to Run

### 1. BPE Training & Evaluation
```bash
jupyter notebook notebooks/bpe_evaluation.ipynb
```

### 2. Train Word2Vec
```bash
python word2vec/train_word2vec.py
```

### 3. Evaluate Word2Vec
```bash
jupyter notebook notebooks/word2vec_evaluation.ipynb
```

### 4. Train BiLSTM LM
```bash
jupyter notebook notebooks/bilstm_language_model.ipynb
```

### 5. Evaluate BiLSTM LM
```bash
jupyter notebook notebooks/bilstm_evaluation.ipynb
```

### 6. Run Integrated Pipeline
```bash
python main.py
```

---

## ğŸ“Š Results (Summary)

### ğŸ”¹ BPE Tokenizer
- Reduced vocab size  
- Low OOV rate  
- Good Jaccard similarity  

### ğŸ”¹ Word2Vec
- Learned semantic clusters (e.g., `def â†” return`, `(` â†” `)`)  

### ğŸ”¹ BiLSTM
- Achieved reasonable perplexity  
- Generated syntactically valid Python snippets  

ğŸ“Œ Detailed metrics, plots, and generated samples are available inside the `notebooks/` folder.
